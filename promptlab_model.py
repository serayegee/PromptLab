# promptlab_model.py 
import pandas as pd
import numpy as np
import json
import re
import os
from typing import Dict, List, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import chromadb

print("üéØ PROMPTLAB FINAL - Gemini RAG Sistemi y√ºkleniyor...")

# Gemini API - opsiyonel
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("‚úÖ Gemini API k√ºt√ºphanesi bulundu")
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è  Gemini API y√ºkl√º deƒüil")

# ==================== VECTOR STORE ====================
class VectorStore:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=5000)
        self.documents = []
        self.metadatas = []
        
        try:
            self.client = chromadb.EphemeralClient()
            self.collection = self.client.create_collection("prompt_examples")
        except:
            self.collection = None
    
    def add_prompts(self, prompts_data):
        self.documents = []
        self.metadatas = []
        ids = []
        
        for i, prompt_info in enumerate(prompts_data):
            self.documents.append(prompt_info['prompt'])
            self.metadatas.append({
                'act': prompt_info.get('act', 'general'),
                'type': prompt_info.get('type', 'general')
            })
            ids.append(str(i))
        
        try:
            if self.collection:
                self.collection.add(
                    documents=self.documents,
                    metadatas=self.metadatas,
                    ids=ids
                )
        except:
            pass
        
        if self.documents:
            try:
                self.vectorizer.fit(self.documents)
            except:
                pass
    
    def search_similar_prompts(self, query: str, n_results: int = 3):
        try:
            if self.collection and self.documents:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=min(n_results, len(self.documents))
                )
                return results
        except:
            pass
        
        # TF-IDF fallback
        try:
            if self.documents and hasattr(self.vectorizer, 'vocabulary_'):
                query_vec = self.vectorizer.transform([query])
                doc_vecs = self.vectorizer.transform(self.documents)
                similarities = cosine_similarity(query_vec, doc_vecs).flatten()
                
                top_indices = similarities.argsort()[-n_results:][::-1]
                return {
                    'documents': [[self.documents[i] for i in top_indices]],
                    'metadatas': [[self.metadatas[i] for i in top_indices]],
                    'distances': [[1 - similarities[i] for i in top_indices]]
                }
        except:
            pass
        
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}

# ==================== GEMINI RAG AGENT ====================
class GeminiRAGAgent:
    def __init__(self, api_key: str = None):
        self.available = False
        self.model = None
        
        if not GEMINI_AVAILABLE:
            return
        
        api_key = api_key or os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            return
        
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
            self.available = True
            print("‚úÖ Gemini RAG Agent hazƒ±r!")
        except Exception as e:
            print(f"‚ùå Gemini hatasƒ±: {e}")
            self.available = False
    
    def generate_optimized_prompt(self, user_prompt: str, similar_examples: List[str], context: Dict) -> str:
        if not self.available:
            return None
        
        rag_prompt = self._build_rag_prompt(user_prompt, similar_examples, context)
        
        try:
            response = self.model.generate_content(
                rag_prompt,
                generation_config={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'max_output_tokens': 1024,
                }
            )
            return response.text.strip()
        except Exception as e:
            print(f"‚ùå Gemini generation hatasƒ±: {e}")
            return None
    
    def _build_rag_prompt(self, user_prompt: str, similar_examples: List[str], context: Dict) -> str:
        examples_text = "\n\n".join([
            f"√ñrnek {i+1}:\n{example}"
            for i, example in enumerate(similar_examples[:3])
        ])
        
        category = context.get('category', 'genel')
        intent = context.get('intent', 'genel')
        
        return f"""Sen bir prompt optimizasyon uzmanƒ±sƒ±n. G√∂revin kullanƒ±cƒ±nƒ±n basit prompt'unu, profesyonel ve etkili bir prompt'a d√∂n√º≈üt√ºrmek.

KULLANICI PROMPT'U:
"{user_prompt}"

PROMPT ANALƒ∞Zƒ∞:
- Kategori: {category}
- Ama√ß: {intent}

BENZER BA≈ûARILI PROMPT √ñRNEKLERƒ∞:
{examples_text}

G√ñREV:
Yukarƒ±daki ba≈üarƒ±lƒ± √∂rnekleri referans alarak, kullanƒ±cƒ±nƒ±n prompt'unu optimize et.

KURALLAR:
1. Orijinal prompt'un amacƒ±nƒ± koru
2. Benzer √∂rneklerdeki yapƒ±yƒ± ve stili uygula
3. Net, detaylƒ± ve actionable bir prompt yaz
4. T√ºrk√ße kullan (orijinal T√ºrk√ße ise)
5. Uzun a√ßƒ±klamalar yapma, direkt optimize edilmi≈ü prompt'u ver

OPTƒ∞Mƒ∞ZE EDƒ∞LMƒ∞≈û PROMPT:"""

# ==================== PROMPT ANALYZER ====================
class PromptAnalyzer:
    def analyze_prompt(self, prompt: str) -> Dict:
        word_count = len(prompt.split())
        
        return {
            'length_score': 0.4 if word_count < 10 else 0.7,
            'specificity_score': 0.3,
            'overall_score': 0.4,
            'issues': ['√áok kƒ±sa', 'Detay eksik'] if word_count < 10 else [],
            'word_count': word_count,
            'category': self._detect_category(prompt),
            'intent': self._detect_intent(prompt)
        }
    
    def _detect_intent(self, prompt: str) -> str:
        prompt_lower = prompt.lower()
        
        if any(kw in prompt_lower for kw in ['√∂ƒüret', 'nasƒ±l', 'anlat', 'a√ßƒ±kla']):
            return 'teaching'
        elif any(kw in prompt_lower for kw in ['sen bir', 'rol√ºnde']):
            return 'role_play'
        elif any(kw in prompt_lower for kw in ['kod yaz', 'program']):
            return 'code_generation'
        elif any(kw in prompt_lower for kw in ['yaz', 'olu≈ütur']):
            return 'content_creation'
        
        return 'general'
    
    def _detect_category(self, prompt: str) -> str:
        intent = self._detect_intent(prompt)
        
        if intent == 'teaching':
            return '√∂ƒüretim'
        elif intent == 'role_play':
            return 'rol oynama'
        elif intent == 'code_generation':
            return 'kod yazma'
        elif intent == 'content_creation':
            return 'i√ßerik olu≈üturma'
        
        return 'genel'

# ==================== FALLBACK OPTIMIZER ====================
class FallbackOptimizer:
    def optimize_prompt(self, original: str, similar_examples: List[str], context: Dict) -> str:
        topic = self._extract_topic(original)
        intent = context.get('intent', 'general')
        
        if similar_examples:
            template = similar_examples[0]
            if 'Sen bir' in template:
                return f"Sen bir {topic} uzmanƒ±sƒ±n. {template.split('.', 1)[1] if '.' in template else 'Kullanƒ±cƒ±ya yardƒ±mcƒ± ol.'}"
        
        if intent == 'teaching':
            return f"Sen bir {topic} √∂ƒüretmenisin. Kullanƒ±cƒ±ya {topic} konusunu adƒ±m adƒ±m √∂ƒüret. √ñrnekler ver, pratik alƒ±≈ütƒ±rmalar sun."
        else:
            return f"{topic.capitalize()} hakkƒ±nda detaylƒ± ve kapsamlƒ± bir yanƒ±t ver. √ñrnekler kullan, net a√ßƒ±klamalar yap."
    
    def _extract_topic(self, prompt: str) -> str:
        stop_words = ['sen', 'bir', 'bana', 'hakkƒ±nda', 'yaz', '√∂ƒüret']
        words = [w for w in prompt.split() if w.lower() not in stop_words and len(w) > 2]
        return ' '.join(words[:2]) if words else 'konu'

# ==================== ANA RAG PIPELINE ====================
class GeminiPromptLabRAG:
    def __init__(self, gemini_api_key: str = None):
        print("üéØ PromptLab RAG Pipeline ba≈ülatƒ±lƒ±yor...")
        
        self.analyzer = PromptAnalyzer()
        self.vector_store = VectorStore()
        self.gemini_agent = GeminiRAGAgent(gemini_api_key)
        self.fallback_optimizer = FallbackOptimizer()
        
        self._load_dataset()
        
        mode = "Gemini RAG" if self.gemini_agent.available else "Fallback"
        print(f"‚úÖ PROMPTLAB HAZIR! (Mod: {mode})")
    
    def _load_dataset(self):
        dataset = [
            {
                "act": "Python Teacher",
                "prompt": "Sen bir Python √∂ƒüretmenisin. √ñƒürencilere Python'ƒ± adƒ±m adƒ±m √∂ƒüret. Temel syntax'tan ba≈üla, her kavramƒ± √∂rneklerle a√ßƒ±kla, kod alƒ±≈ütƒ±rmalarƒ± ver. Sabƒ±rlƒ±, net ve te≈üvik edici ol.",
                "type": "teaching"
            },
            {
                "act": "Code Reviewer",
                "prompt": "Sen bir senior developer'sƒ±n. Kodu incele: okunabilirlik, performans, g√ºvenlik, best practices. Yapƒ±cƒ± geri bildirim ver, alternatif √ß√∂z√ºmler √∂ner.",
                "type": "coding"
            },
            {
                "act": "Academic Writer",
                "prompt": "Sen bir akademik yazarsƒ±n. Yapƒ±: Giri≈ü (baƒülam, tez), Geli≈üme (arg√ºmanlar, kanƒ±tlar), Sonu√ß (√∂zet, √ßƒ±karƒ±mlar). Formal dil, kaynak g√∂ster, objektif ol.",
                "type": "writing"
            },
            {
                "act": "Math Teacher",
                "prompt": "Sen bir matematik √∂ƒüretmenisin. Konsepti a√ßƒ±kla, adƒ±m adƒ±m √ß√∂z, alternatif y√∂ntemler g√∂ster, pratik problemler ver. G√∂rsel yardƒ±mcƒ±lar kullan.",
                "type": "teaching"
            },
            {
                "act": "Business Analyst",
                "prompt": "Sen bir i≈ü analistisin. Veri analizi yap, KPI'lar tanƒ±mla, insights sun, iyile≈ütirme √∂nerileri getir. SWOT, market research kullan.",
                "type": "business"
            },
            {
                "act": "Content Writer",
                "prompt": "Sen bir i√ßerik yazarƒ±sƒ±n. SEO-friendly, engaging i√ßerik yaz. Net ba≈ülƒ±klar, deƒüer kat, hedef kitleye uygun ton, CTA ekle.",
                "type": "writing"
            },
            {
                "act": "UX Designer",
                "prompt": "Sen bir UX tasarƒ±mcƒ±sƒ±sƒ±n. User-centered design yap, wireframes olu≈ütur, usability test et. Accessibility standartlarƒ±na uy.",
                "type": "design"
            },
            {
                "act": "Data Scientist",
                "prompt": "Sen bir veri bilimcisin. Veri analizi yap, ML modelleri olu≈ütur, istatistiksel analiz ger√ßekle≈ütir. Python kullan, g√∂rselle≈ütir.",
                "type": "data"
            }
        ]
        
        self.vector_store.add_prompts(dataset)
    
    def process_prompt(self, user_prompt: str) -> Dict:
        """Ana RAG i≈ülemi - HER ZAMAN rag_mode d√∂nd√ºr√ºr"""
        
        # 1. ANALYZE
        analysis = self.analyzer.analyze_prompt(user_prompt)
        
        # 2. RETRIEVE
        similar_prompts = self.vector_store.search_similar_prompts(user_prompt, n_results=3)
        similar_examples = similar_prompts['documents'][0] if similar_prompts['documents'][0] else []
        
        # 3. GENERATE
        if self.gemini_agent.available:
            optimized = self.gemini_agent.generate_optimized_prompt(
                user_prompt, similar_examples, analysis
            )
            
            if optimized:
                ai_model = "Gemini Pro (RAG)"
                rag_mode = "Gemini RAG"
            else:
                optimized = self.fallback_optimizer.optimize_prompt(
                    user_prompt, similar_examples, analysis
                )
                ai_model = "Fallback Optimizer"
                rag_mode = "Fallback"
        else:
            optimized = self.fallback_optimizer.optimize_prompt(
                user_prompt, similar_examples, analysis
            )
            ai_model = "Fallback Optimizer (Gemini N/A)"
            rag_mode = "Fallback"
        
        improvement = max(0, (1.0 - analysis['overall_score']) * 100)
        
        # RETURN - rag_mode her zaman var!
        return {
            'original_prompt': user_prompt,
            'analysis': analysis,
            'similar_examples': similar_prompts,
            'optimized_prompt': optimized,
            'improvement_percentage': improvement,
            'generative_ai_used': self.gemini_agent.available,
            'ai_model': ai_model,
            'rag_mode': rag_mode  # ‚Üê BU HER ZAMAN VAR!
        }

# ==================== GLOBAL INSTANCE ====================
GEMINI_KEY = os.environ.get('GEMINI_API_KEY')

try:
    promptlab = GeminiPromptLabRAG(gemini_api_key=GEMINI_KEY)
    print("‚úÖ PromptLab global instance olu≈üturuldu")
except Exception as e:
    print(f"‚ùå PromptLab hatasƒ±: {e}")
    promptlab = None

if __name__ == "__main__":
    print("\nüß™ Test ba≈ülƒ±yor...")
    if promptlab:
        result = promptlab.process_prompt("test prompt")
        print(f"‚úÖ Test ba≈üarƒ±lƒ±!")
        print(f"   Keys: {list(result.keys())}")
        print(f"   RAG Mode: {result['rag_mode']}")
    else:
        print("‚ùå PromptLab y√ºklenemedi!")